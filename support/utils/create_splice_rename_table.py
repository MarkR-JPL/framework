#!/usr/bin/env python

# Using the input XML file from the official L2 Aggregator, create a mapping of dataset names
# coming out of the L2 full physics files to what they would be in the official aggregated files.
# Requires as argument the following files from the L2AggPGE repo:
# * oco2_L2Agg_apf_140820143657.xml        
# * oco2_L2Dia_OutHDFDef_140825213246.xml
# Additionally the xsd file is needed:
# * oco_parameterfile_060401120000.xsd
# 
# L2AggPGE repo:
# https://svn.jpl.nasa.gov/trac/browser/sdos/oco2/trunk/PGE/L2AggPGE/tables

import os
import sys

from pprint import pprint
import xml.etree.ElementTree as ET

apf_fn = sys.argv[1]
apf_file = ET.parse(apf_fn)

hdf_def_fn = sys.argv[2]
hdf_def_file = ET.parse(hdf_def_fn)

mapping = {}

# Come up with which apf data to search from
search_groups = [
        "ScienceH5FileStateVectorFields",
        "ScienceH5FileRetrievalFields", 
        "SubsettedL1BFieldNames",
        "SubsettedL1BFrameFieldNames",
        "SubsettedDOASFieldNames",
        "SubsettedCloudFieldNames",
        ]

search_fields = []
for grp_name in search_groups:
    srch1 = apf_file.find("*[@name='%s']" % grp_name)
    if srch1 != None:
        search_fields += srch1
    else:
        srch2 = apf_file.find("*/*[@name='%s']" % grp_name)

        if srch2 != None:
            search_fields += srch2
        else:
            raise Exception("Could not find group: %s" % grp_name)

# Figure out mapping from HDF XML file
for field in search_fields:
    src_name = field.text
    dst_name = field.attrib["name"]

    field_info =  hdf_def_file.find("*/*[@name='%s']" % dst_name)
    dst_shape = field_info.find("*[@name='Shape']").text

    field_group = hdf_def_file.find("*/*[@name='%s'].." % dst_name)
    group_name = field_group.attrib["name"]


    mapping[src_name] = { "name": "%s/%s" % (group_name, dst_name), "shape": dst_shape }

# Get list of all dataset names known by the aggregator
agg_dataset_names = []
for group_elem in hdf_def_file.findall('group'):
    group_name = group_elem.attrib['name']
    for dataset_elem in group_elem.findall('group'):
        ds_name = dataset_elem.attrib['name']
        agg_dataset_names.append( '%s/%s' % (group_name, ds_name))

print "# Generated by: %s" % os.path.basename(sys.argv[0])
print "# From: %s, %s" % (os.path.basename(apf_fn), os.path.basename(hdf_def_fn))
print "aggregator_dataset_mapping = \\"
pprint(mapping)
print ""
print "aggregator_dataset_dest_names = \\"
pprint(agg_dataset_names)
